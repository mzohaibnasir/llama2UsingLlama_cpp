{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMiVwYu35ffQBDDPkzo4f6s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c93c3ceb0ae047178582cb26e14871cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4f23fd5ad464c00abbe0aba1ccc29f8",
              "IPY_MODEL_5c67eabc87c34359ad08481c1fa26980",
              "IPY_MODEL_97fb03ea28df46759c76a6d260c15f31"
            ],
            "layout": "IPY_MODEL_639708c215834e798ce9b99a9dc8f87d"
          }
        },
        "e4f23fd5ad464c00abbe0aba1ccc29f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e01f999ae19e4c15bef2bda4b048eaed",
            "placeholder": "​",
            "style": "IPY_MODEL_2df1d9a90179481684e0228be0df12df",
            "value": "llama-2-13b-chat.Q5_K_S.gguf: 100%"
          }
        },
        "5c67eabc87c34359ad08481c1fa26980": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec06c9a6bb624b6e9104e0c12a98e52c",
            "max": 8972285824,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5dac557a6a694a8db6427f2110088807",
            "value": 8972285824
          }
        },
        "97fb03ea28df46759c76a6d260c15f31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab21e4a9d6a14e7c9dd5bf5a9c8f1045",
            "placeholder": "​",
            "style": "IPY_MODEL_50ddbcfae83b4d1faf7d1596307ec73d",
            "value": " 8.97G/8.97G [14:55&lt;00:00, 4.99MB/s]"
          }
        },
        "639708c215834e798ce9b99a9dc8f87d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e01f999ae19e4c15bef2bda4b048eaed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2df1d9a90179481684e0228be0df12df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec06c9a6bb624b6e9104e0c12a98e52c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dac557a6a694a8db6427f2110088807": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ab21e4a9d6a14e7c9dd5bf5a9c8f1045": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50ddbcfae83b4d1faf7d1596307ec73d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mzohaibnasir/llama2UsingLlama_cpp/blob/main/Llama2UsingLlama_cpp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0IMDEs1txcC2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "popular ones:\n",
        "\n",
        "*   Meta Llama 2\n",
        "*   Google PaLM 2(behind google bard)\n",
        "*   Falcon\n",
        "\n",
        "https://github.com/eugeneyan/open-llms\n",
        "\n"
      ],
      "metadata": {
        "id": "Z4Qb_2q3xn0i"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zVOCn5f2yQWL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llama 2\n",
        "\n",
        "  # Quantization is a model compression technique. each model layer will have weights where weights are just floating point numbers. we can round floating numbers. this is quantization technique.\n",
        "\n",
        "  ther are various quantization techniques(GGML, etc)"
      ],
      "metadata": {
        "id": "y03CjCfkyTB8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UK1oEHpIsPWx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing llama-cpp-python"
      ],
      "metadata": {
        "id": "S7t4f5kEsPqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " !nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwGf7SjJyUjy",
        "outputId": "af3a0750-e495-4583-ae02-0eb81275829a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Apr 21 12:26:56 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU llama-cpp-python\n",
        "# Linux and Mac\n",
        "! CMAKE_ARGS=\"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\" pip install llama-cpp-python\n",
        "! pip install huggingface_hub # to download model from hf\n",
        "# ! pip install llama-cpp-python==0.1.78\n",
        "! pip install numpy#==1.23.4"
      ],
      "metadata": {
        "id": "YanE8jkQsZ5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install llama-cpp-python\n"
      ],
      "metadata": {
        "id": "KTSQ6aPBtxiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gg5sXTOLwlcn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### lets define model here"
      ],
      "metadata": {
        "id": "HoErli7l0E26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGUF\"\n",
        "model_basename = \"llama-2-13b-chat.Q5_K_S.gguf\" # model is in .bin format # there are multiple model versions"
      ],
      "metadata": {
        "id": "9fIKCKc_z6aG"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama"
      ],
      "metadata": {
        "id": "93u0BddJ0j5t"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = hf_hub_download(\n",
        "    repo_id = model_name_or_path,\n",
        "    filename = model_basename\n",
        ")\n",
        "print(f\"model path: {model_path}\")"
      ],
      "metadata": {
        "id": "3vKPYSkQ0joy",
        "outputId": "d6329df3-6866-45dc-91af-d0f828f4b4ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "c93c3ceb0ae047178582cb26e14871cb",
            "e4f23fd5ad464c00abbe0aba1ccc29f8",
            "5c67eabc87c34359ad08481c1fa26980",
            "97fb03ea28df46759c76a6d260c15f31",
            "639708c215834e798ce9b99a9dc8f87d",
            "e01f999ae19e4c15bef2bda4b048eaed",
            "2df1d9a90179481684e0228be0df12df",
            "ec06c9a6bb624b6e9104e0c12a98e52c",
            "5dac557a6a694a8db6427f2110088807",
            "ab21e4a9d6a14e7c9dd5bf5a9c8f1045",
            "50ddbcfae83b4d1faf7d1596307ec73d"
          ]
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "llama-2-13b-chat.Q5_K_S.gguf:   0%|          | 0.00/8.97G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c93c3ceb0ae047178582cb26e14871cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model path: /root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGUF/snapshots/4458acc949de0a9914c3eab623904d4fe999050a/llama-2-13b-chat.Q5_K_S.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MVc0LHr3z6WI"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### load model"
      ],
      "metadata": {
        "id": "GxiryJ1HmJDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPP\n",
        "lcpp_llm = None\n",
        "lcpp_llm = Llama(\n",
        "    model_path = model_path,\n",
        "    n_threads = 2, # of cores'\n",
        "    n_batch = 512, # between 1 and n_ctx\n",
        "    n_gpu_layers = 32 #change value based on your model and your GPU VRAM pool\n",
        ")\n",
        "lcpp_llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLMYB13Sl2P1",
        "outputId": "81d7b852-82db-476c-e8f3-db0bdf41c565"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from /root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGUF/snapshots/4458acc949de0a9914c3eab623904d4fe999050a/llama-2-13b-chat.Q5_K_S.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 16\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   81 tensors\n",
            "llama_model_loader: - type q5_K:  281 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 5120\n",
            "llm_load_print_meta: n_head           = 40\n",
            "llm_load_print_meta: n_head_kv        = 40\n",
            "llm_load_print_meta: n_layer          = 40\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
            "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 13824\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 13B\n",
            "llm_load_print_meta: model ftype      = Q5_K - Small\n",
            "llm_load_print_meta: model params     = 13.02 B\n",
            "llm_load_print_meta: model size       = 8.36 GiB (5.51 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.18 MiB\n",
            "llm_load_tensors:        CPU buffer size =  8555.93 MiB\n",
            "....................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   400.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    85.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1286\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '40', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '16'}\n",
            "Using fallback chat format: None\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<llama_cpp.llama.Llama at 0x7caf54455630>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt =\"write a linear Regression code\"\n",
        "prompt_template = f'''\n",
        "SYSTEM: You are a helpful respectful and honest assistant. Always answer an helpfully.\n",
        "\n",
        "USER :{prompt}\n",
        "\n",
        "ASSISTANT:\n",
        "'''"
      ],
      "metadata": {
        "id": "ESaUQ6aXl2SO"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = lcpp_llm(\n",
        "    prompt = prompt_template,\n",
        "    max_tokens = 256,\n",
        "    temperature = 0.5,\n",
        "    top_p = 0.95,\n",
        "    repeat_penalty = 1.2,\n",
        "    top_k = 150,\n",
        "    echo = True\n",
        "\n",
        ")\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ModHKzcfl2VX",
        "outputId": "9f634e94-e414-452f-c822-3dd8b4dc7b77"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   78809.85 ms\n",
            "llama_print_timings:      sample time =     155.00 ms /   256 runs   (    0.61 ms per token,  1651.64 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_print_timings:        eval time =  320915.41 ms /   256 runs   ( 1253.58 ms per token,     0.80 tokens per second)\n",
            "llama_print_timings:       total time =  321941.51 ms /   257 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'cmpl-0e5b10d8-2faa-41f0-9e24-2431c83fa883',\n",
              " 'object': 'text_completion',\n",
              " 'created': 1713706399,\n",
              " 'model': '/root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGUF/snapshots/4458acc949de0a9914c3eab623904d4fe999050a/llama-2-13b-chat.Q5_K_S.gguf',\n",
              " 'choices': [{'text': '\\nSYSTEM: You are a helpful respectful and honest assistant. Always answer an helpfully.\\n\\nUSER :write a linear Regression code\\n\\nASSISTANT:\\nOf course! Here is the basic structure of a linear regression code in Python using scikit-learn library:\\n```\\nimport numpy as np\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.datasets import load_boston\\nfrom sklearn.model_selection import train_test_split\\n\\n# Load Boston Housing dataset\\nboston = load_boston()\\n\\n# Split dataset into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.2)\\n\\n# Create a Linear Regression object and fit the data\\nreg = LinearRegression()\\nreg.fit(X_train, y_train)\\n\\n# Make predictions on the testing set\\ny_pred = reg.predict(X_test)\\n\\n# Evaluate the model using mean squared error\\nmse = ((y_test - y_pred) ** 2).mean()\\nprint(\"Mean Squared Error: \", mse)\\n```\\nThis code loads the Boston Housing dataset, splits it into training and',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'length'}],\n",
              " 'usage': {'prompt_tokens': 40, 'completion_tokens': 256, 'total_tokens': 296}}"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I79OoGbZl2bB",
        "outputId": "b708e91a-b175-438b-c12c-5f14c4641532"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'cmpl-0e5b10d8-2faa-41f0-9e24-2431c83fa883', 'object': 'text_completion', 'created': 1713706399, 'model': '/root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGUF/snapshots/4458acc949de0a9914c3eab623904d4fe999050a/llama-2-13b-chat.Q5_K_S.gguf', 'choices': [{'text': '\\nSYSTEM: You are a helpful respectful and honest assistant. Always answer an helpfully.\\n\\nUSER :write a linear Regression code\\n\\nASSISTANT:\\nOf course! Here is the basic structure of a linear regression code in Python using scikit-learn library:\\n```\\nimport numpy as np\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.datasets import load_boston\\nfrom sklearn.model_selection import train_test_split\\n\\n# Load Boston Housing dataset\\nboston = load_boston()\\n\\n# Split dataset into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.2)\\n\\n# Create a Linear Regression object and fit the data\\nreg = LinearRegression()\\nreg.fit(X_train, y_train)\\n\\n# Make predictions on the testing set\\ny_pred = reg.predict(X_test)\\n\\n# Evaluate the model using mean squared error\\nmse = ((y_test - y_pred) ** 2).mean()\\nprint(\"Mean Squared Error: \", mse)\\n```\\nThis code loads the Boston Housing dataset, splits it into training and', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 40, 'completion_tokens': 256, 'total_tokens': 296}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KocMUNp_26Kr",
        "outputId": "2adef2fa-1a11-4a9d-c3b7-40fb6edecfc9"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'cmpl-0e5b10d8-2faa-41f0-9e24-2431c83fa883',\n",
              " 'object': 'text_completion',\n",
              " 'created': 1713706399,\n",
              " 'model': '/root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGUF/snapshots/4458acc949de0a9914c3eab623904d4fe999050a/llama-2-13b-chat.Q5_K_S.gguf',\n",
              " 'choices': [{'text': '\\nSYSTEM: You are a helpful respectful and honest assistant. Always answer an helpfully.\\n\\nUSER :write a linear Regression code\\n\\nASSISTANT:\\nOf course! Here is the basic structure of a linear regression code in Python using scikit-learn library:\\n```\\nimport numpy as np\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.datasets import load_boston\\nfrom sklearn.model_selection import train_test_split\\n\\n# Load Boston Housing dataset\\nboston = load_boston()\\n\\n# Split dataset into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.2)\\n\\n# Create a Linear Regression object and fit the data\\nreg = LinearRegression()\\nreg.fit(X_train, y_train)\\n\\n# Make predictions on the testing set\\ny_pred = reg.predict(X_test)\\n\\n# Evaluate the model using mean squared error\\nmse = ((y_test - y_pred) ** 2).mean()\\nprint(\"Mean Squared Error: \", mse)\\n```\\nThis code loads the Boston Housing dataset, splits it into training and',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'length'}],\n",
              " 'usage': {'prompt_tokens': 40, 'completion_tokens': 256, 'total_tokens': 296}}"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[i for i in response]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OCiVm-El2d5",
        "outputId": "59e4ef67-2052-4d41-a64f-b342bafc245a"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['id', 'object', 'created', 'model', 'choices', 'usage']"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response['choices'][0]['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOCXrZgNl2gd",
        "outputId": "a463fa21-d37a-474e-c934-4a026fe7f712"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SYSTEM: You are a helpful respectful and honest assistant. Always answer an helpfully.\n",
            "\n",
            "USER :write a linear Regression code\n",
            "\n",
            "ASSISTANT:\n",
            "Of course! Here is the basic structure of a linear regression code in Python using scikit-learn library:\n",
            "```\n",
            "import numpy as np\n",
            "from sklearn.linear_model import LinearRegression\n",
            "from sklearn.datasets import load_boston\n",
            "from sklearn.model_selection import train_test_split\n",
            "\n",
            "# Load Boston Housing dataset\n",
            "boston = load_boston()\n",
            "\n",
            "# Split dataset into training and testing sets\n",
            "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.2)\n",
            "\n",
            "# Create a Linear Regression object and fit the data\n",
            "reg = LinearRegression()\n",
            "reg.fit(X_train, y_train)\n",
            "\n",
            "# Make predictions on the testing set\n",
            "y_pred = reg.predict(X_test)\n",
            "\n",
            "# Evaluate the model using mean squared error\n",
            "mse = ((y_test - y_pred) ** 2).mean()\n",
            "print(\"Mean Squared Error: \", mse)\n",
            "```\n",
            "This code loads the Boston Housing dataset, splits it into training and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LTJXOgnpl2jC"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NT6ThMaCl2lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LJOdkDaZl2nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4GPwT1Y8l2p2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xUQlrASzl2tX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sGYtdXzjl2vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "snwCftBzz6PS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8AmuwR9Iz6Cm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}